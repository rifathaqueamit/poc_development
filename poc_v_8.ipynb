{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Demonstration of the POC for searching content inside video</h1>"
      ],
      "metadata": {
        "id": "Had2ogKlkiJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisites\n",
        "\n",
        "\n",
        "1.  Create a folder with name \"input_videos\" and put your video files as mp4\n",
        "2.  Upload the action_model.tflite file\n",
        "3.  Upload the object_model.tflite file\n",
        "4.  Go through steps one by one\n",
        "\n",
        "Update : V8 (19th Oct, 2023)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1-wWKf8hmmqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s3khsunT7kWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06cc5d4a-0395-4c41-838a-9cafb8c4570e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title 1. Install dependencies\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dI_1csl6Q-gH"
      },
      "outputs": [],
      "source": [
        "# @title 2. Import dependencies\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "import PIL\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tqdm\n",
        "import absl.logging\n",
        "import random\n",
        "import re\n",
        "import tempfile\n",
        "import ssl\n",
        "import cv2\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "mpl.rcParams.update({\n",
        "    'font.size': 10,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Define constants (if required)\n",
        "#Common constants\n",
        "input_videos_path = \"input_videos\"\n",
        "output_videos_path = \"output_videos\"\n",
        "video_fps = 5\n",
        "seconds_per_segments = 5\n",
        "threshold = 50\n",
        "frames_per_video = seconds_per_segments * video_fps\n",
        "\n",
        "#Action detection\n",
        "action_frame_width = 224\n",
        "action_frame_height = 224\n",
        "\n",
        "#Object detection\n",
        "object_frame_width = 320\n",
        "object_frame_height = 320\n",
        "class_names = [\n",
        "      \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\",\n",
        "      \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
        "      \"fire hydrant\", \"---\", \"stop sign\", \"parking meter\", \"bench\",\n",
        "      \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
        "      \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"---\", \"backpack\",\n",
        "      \"umbrella\", \"---\", \"---\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
        "      \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
        "      \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
        "      \"bottle\", \"---\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\",\n",
        "      \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
        "      \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "      \"couch\", \"potted plant\", \"bed\", \"---\",  \"dining table\", \"---\", \"---\",\n",
        "      \"toilet\", \"---\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
        "      \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"---\", \"book\",\n",
        "      \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\", \"---\"\n",
        "  ]"
      ],
      "metadata": {
        "id": "nFMSuU4aoXIx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dx55NK3ZoZeh"
      },
      "outputs": [],
      "source": [
        "# @title 4. Helper functions\n",
        "\n",
        "# Download Kinetics 600 label map\n",
        "!wget https://raw.githubusercontent.com/tensorflow/models/f8af2291cced43fc9f1d9b41ddbf772ae7b0d7d2/official/projects/movinet/files/kinetics_600_labels.txt -O labels.txt -q\n",
        "\n",
        "with tf.io.gfile.GFile('labels.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  KINETICS_600_LABELS_LIST = [line.strip() for line in lines]\n",
        "  KINETICS_600_LABELS = tf.constant(KINETICS_600_LABELS_LIST)\n",
        "\n",
        "def get_top_k(probs, k=5, label_map=KINETICS_600_LABELS):\n",
        "  \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\"\n",
        "  top_predictions = tf.argsort(probs, axis=-1, direction='DESCENDING')[:k]\n",
        "  top_labels = tf.gather(label_map, top_predictions, axis=-1)\n",
        "  top_labels = [label.decode('utf8') for label in top_labels.numpy()]\n",
        "  top_probs = tf.gather(probs, top_predictions, axis=-1).numpy()\n",
        "  return tuple(zip(top_labels, top_probs))\n",
        "\n",
        "# Utilities to open video files using CV2\n",
        "def crop_center_square(frame):\n",
        "  y, x = frame.shape[0:2]\n",
        "  min_dim = min(y, x)\n",
        "  start_x = (x // 2) - (min_dim // 2)\n",
        "  start_y = (y // 2) - (min_dim // 2)\n",
        "  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(object_frame_width, object_frame_height)):\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  try:\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      frame = crop_center_square(frame)\n",
        "      frame = cv2.resize(frame, resize)\n",
        "      frame = frame[:, :, [2, 1, 0]]\n",
        "      frames.append(frame)\n",
        "\n",
        "      if len(frames) == max_frames:\n",
        "        break\n",
        "  finally:\n",
        "    cap.release()\n",
        "  return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Load pre-trained model\n",
        "# Create the interpreter and signature runner\n",
        "action_interpreter = tf.lite.Interpreter(model_path=\"action_model.tflite\")\n",
        "action_runner = action_interpreter.get_signature_runner()\n",
        "\n",
        "object_interpreter = tf.lite.Interpreter(model_path=\"object_model.tflite\")\n",
        "object_interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "LlfrEvXvZFEV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Make necessary folders\n",
        "if os.path.exists(input_videos_path) == False:\n",
        "  os.mkdir(input_videos_path)\n",
        "\n",
        "if os.path.exists(output_videos_path) == False:\n",
        "  os.mkdir(output_videos_path)"
      ],
      "metadata": {
        "id": "iaB-0PcKxlRz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Collect input videos (Mp4 only)\n",
        "input_videos = []\n",
        "\n",
        "for file in os.listdir(\"input_videos\"):\n",
        "    if file.endswith(\".mp4\"):\n",
        "        input_videos.append(file)\n",
        "\n",
        "if len(input_videos) == 0:\n",
        "  print(\"There is no input videos\")\n",
        "else:\n",
        "  print(input_videos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY4FGs4xwnmU",
        "outputId": "045c30c7-1581-40c3-831f-7492a940f7d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['video1.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Convert the input mp4 video into AVI format\n",
        "output_videos = []\n",
        "for input_video in input_videos:\n",
        "  input_path = input_videos_path + \"/\" + input_video\n",
        "  path, filename_ext = os.path.split(input_path)\n",
        "  filename = os.path.splitext(filename_ext)\n",
        "  output_path = output_videos_path + \"/\" + filename[0] + \".avi\"\n",
        "  if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "  print(\"Input : \", input_path)\n",
        "  print(\"Output : \", output_path)\n",
        "  !ffmpeg -i $input_path -filter:v fps=$video_fps $output_path\n",
        "  output_videos.append(output_path)"
      ],
      "metadata": {
        "id": "oCHpfTsljZJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Load the videos and show\n",
        "action_videos = []\n",
        "object_videos = []\n",
        "for url in output_videos:\n",
        "  object_video = load_video(url)\n",
        "  action_video = load_video(url, resize=(action_frame_width, action_frame_height)) / 255\n",
        "  # media.show_video(loaded_video, fps=video_fps)\n",
        "\n",
        "  print(\"path : \", url)\n",
        "  print(\"action video frames count, width, height, color : \", action_video.shape)\n",
        "  print(\"object video frames count, width, height, color : \", object_video.shape)\n",
        "\n",
        "  action_videos.append((url, action_video))\n",
        "  object_videos.append((url, object_video))"
      ],
      "metadata": {
        "id": "KBh2njQJaCZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa31a3d2-0eec-4869-dd72-dd81a3ce25c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path :  output_videos/video1.avi\n",
            "action video frames count, width, height, color :  (77, 224, 224, 3)\n",
            "object video frames count, width, height, color :  (77, 320, 320, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Separate the videos in small segments\n",
        "def separateVideoInSegments(url, video_frames):\n",
        "  print(\"separateVideoInSegments() : \", url)\n",
        "  video_segments = []\n",
        "  video_segments_playable = []\n",
        "\n",
        "  for idx, frame in enumerate(video_frames):\n",
        "    video_id = int(idx / frames_per_video)\n",
        "    try:\n",
        "      f32_frame = tf.cast(frame, tf.float32)\n",
        "      video_segments[video_id].append(f32_frame)\n",
        "      video_segments_playable[video_id].append(frame)\n",
        "    except IndexError:\n",
        "      video_segments.append([f32_frame])\n",
        "      video_segments_playable.append([frame])\n",
        "\n",
        "  for idx, segment in enumerate(video_segments):\n",
        "    print(\"Segment \", idx+1, \", total frames : \", len(segment))\n",
        "\n",
        "  return (url, video_segments, video_segments_playable)"
      ],
      "metadata": {
        "id": "uhE11gzJJui5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_videos_segments = []\n",
        "object_videos_segments = []\n",
        "\n",
        "# Action videos\n",
        "for url, video in action_videos:\n",
        "  action_videos_segments.append(separateVideoInSegments(url, video))\n",
        "\n",
        "for url, video in object_videos:\n",
        "  object_videos_segments.append(separateVideoInSegments(url, video))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVSq6Ler0nIJ",
        "outputId": "b40453e0-0860-4be5-c802-8fdecb4426ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "separateVideoInSegments() :  output_videos/video1.avi\n",
            "Segment  1 , total frames :  25\n",
            "Segment  2 , total frames :  25\n",
            "Segment  3 , total frames :  25\n",
            "Segment  4 , total frames :  2\n",
            "separateVideoInSegments() :  output_videos/video1.avi\n",
            "Segment  1 , total frames :  25\n",
            "Segment  2 , total frames :  25\n",
            "Segment  3 , total frames :  25\n",
            "Segment  4 , total frames :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. Define the detect function\n",
        "def detectFirstPrint(video):\n",
        "  init_states = {\n",
        "    name: tf.zeros(x['shape'], dtype=x['dtype'])\n",
        "    for name, x in action_runner.get_input_details().items()\n",
        "  }\n",
        "  del init_states['image']\n",
        "\n",
        "  clips = video\n",
        "  states = init_states\n",
        "  for clip in clips:\n",
        "    # Input shape: [1, 1, 224, 224, 3]\n",
        "    outputs = action_runner(**states, image=clip)\n",
        "    logits = outputs.pop('logits')[0]\n",
        "    states = outputs\n",
        "\n",
        "  probs = tf.nn.softmax(logits)\n",
        "  top_k = get_top_k(probs)\n",
        "\n",
        "  for label, prob in top_k:\n",
        "    if prob * 100 >= threshold:\n",
        "      print(\"     Action : \" + label + \", with probability : \" + str(prob * 100) + \"%\")\n",
        "      return\n",
        "\n",
        "  print(\"     Action : nothing detected\")"
      ],
      "metadata": {
        "id": "NRxJJ-ZeXNtd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 12. Run inference\n",
        "\n",
        "# Video action classification\n",
        "for video in action_videos_segments:\n",
        "  video_segments = video[1]\n",
        "  video_segments_playable = video[2]\n",
        "\n",
        "  print(\"Url : \", video[0])\n",
        "  print(\".............................\")\n",
        "\n",
        "  segment_id = 0\n",
        "  for segment in video_segments:\n",
        "    segment_start_time = segment_id * seconds_per_segments\n",
        "    segment_end_time = (segment_id + 1) * seconds_per_segments\n",
        "    print(\"Segment : \", segment_id+1, \" , start : \", segment_start_time, \" , end : \", segment_end_time)\n",
        "    detectFirstPrint(video_segments[segment_id])\n",
        "    # media.show_video(video_segments_playable[segment_id], fps=video_fps)\n",
        "    segment_id = segment_id + 1"
      ],
      "metadata": {
        "id": "KTAZfYF05wd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1eef80-7e17-4673-888a-08d758537e02"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Url :  output_videos/video1.avi\n",
            ".............................\n",
            "Segment :  1  , start :  0  , end :  5\n",
            "     Action : salsa dancing, with probability : 97.83188700675964%\n",
            "Segment :  2  , start :  5  , end :  10\n",
            "     Action : salsa dancing, with probability : 88.37751150131226%\n",
            "Segment :  3  , start :  10  , end :  15\n",
            "     Action : nothing detected\n",
            "Segment :  4  , start :  15  , end :  20\n",
            "     Action : nothing detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "object_input_details = object_interpreter.get_input_details()\n",
        "object_output_details = object_interpreter.get_output_details()\n",
        "object_input_shape = object_input_details[0]['shape'][1:3]"
      ],
      "metadata": {
        "id": "UxNPc22s5dGC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detectObjects(video):\n",
        "  for idx, frame in enumerate(video):\n",
        "    if idx % video_fps == 0:\n",
        "      # detected_objects_in_frame = []\n",
        "      #plt.imshow(frame / 255)\n",
        "      #plt.axis('off')\n",
        "      #plt.show()\n",
        "      int_frame = np.array(frame, dtype=np.uint8)\n",
        "      int_frame = np.expand_dims(int_frame, axis=0)\n",
        "      object_interpreter.set_tensor(object_input_details[0]['index'], int_frame)\n",
        "      object_interpreter.invoke()\n",
        "      classes = object_interpreter.get_tensor(object_output_details[1]['index'])\n",
        "      scores = object_interpreter.get_tensor(object_output_details[2]['index'])\n",
        "      num_detections = int(object_interpreter.get_tensor(object_output_details[3]['index']))\n",
        "\n",
        "      detections = set()\n",
        "\n",
        "      for k in range(num_detections):\n",
        "        if scores[0, k] > (threshold / float(100)):\n",
        "            class_id = int(classes[0, k])\n",
        "            class_name = class_names[class_id]\n",
        "            # confidence = scores[0, k]\n",
        "            # label = f\"class name = {class_name}, class id = {class_id + 1}, confidence = {confidence:.2f}\"\n",
        "            # print(label)\n",
        "            # detected_objects_in_frame.append(label)\n",
        "            detections.add(class_name)\n",
        "      print(\"     Frame : \", idx, \", Detections : \", detections)\n"
      ],
      "metadata": {
        "id": "G73-rB6q24K4"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Object detection\n",
        "for video in object_videos_segments:\n",
        "  video_segments = video[1]\n",
        "  video_segments_playable = video[2]\n",
        "\n",
        "  print(\"Url : \", video[0])\n",
        "  print(\".............................\")\n",
        "\n",
        "  segment_id = 0\n",
        "  for segment in video_segments:\n",
        "    segment_start_time = segment_id * seconds_per_segments\n",
        "    segment_end_time = (segment_id + 1) * seconds_per_segments\n",
        "    print(\"Segment : \", segment_id+1, \" , start : \", segment_start_time, \" , end : \", segment_end_time)\n",
        "    detectObjects(video_segments[segment_id])\n",
        "    # media.show_video(video_segments_playable[segment_id], fps=video_fps)\n",
        "    segment_id = segment_id + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MFD6ZIG2lRo",
        "outputId": "bc528ef7-c011-415a-c9f8-798b3fb8f6c0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Url :  output_videos/video1.avi\n",
            ".............................\n",
            "Segment :  1  , start :  0  , end :  5\n",
            "     Frame :  0 , Detections :  {'person'}\n",
            "     Frame :  5 , Detections :  {'person'}\n",
            "     Frame :  10 , Detections :  {'person', 'car'}\n",
            "     Frame :  15 , Detections :  {'person'}\n",
            "     Frame :  20 , Detections :  {'person'}\n",
            "Segment :  2  , start :  5  , end :  10\n",
            "     Frame :  0 , Detections :  {'person'}\n",
            "     Frame :  5 , Detections :  {'person'}\n",
            "     Frame :  10 , Detections :  {'person'}\n",
            "     Frame :  15 , Detections :  {'person'}\n",
            "     Frame :  20 , Detections :  {'person'}\n",
            "Segment :  3  , start :  10  , end :  15\n",
            "     Frame :  0 , Detections :  {'person'}\n",
            "     Frame :  5 , Detections :  {'person', 'car'}\n",
            "     Frame :  10 , Detections :  {'person'}\n",
            "     Frame :  15 , Detections :  {'person'}\n",
            "     Frame :  20 , Detections :  {'person'}\n",
            "Segment :  4  , start :  15  , end :  20\n",
            "     Frame :  0 , Detections :  {'person'}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}